Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	calculate_het
	1	check_relatedness
	1	check_sex
	1	dedeuplicate
	1	filter_het
	1	filter_sex
	1	generate_final_data
	1	prune_ld
	1	qc_base
	1	qc_target
	1	strand_flipping
	12

[Mon Jun 17 14:01:58 2024]
rule qc_target:
    input: data/target/genotype/ukb_imp_allchr_v3_40k_fmri.bim, data/target/genotype/ukb_imp_allchr_v3_40k_fmri.bed, data/target/genotype/ukb_imp_allchr_v3_40k_fmri.fam
    output: results/AN/target/QC/target.QC.fam, results/AN/target/QC/target.QC.snplist
    jobid: 7

Terminating processes on user request, this might take some time.
[Mon Jun 17 14:02:12 2024]
Error in rule qc_target:
    jobid: 7
    output: results/AN/target/QC/target.QC.fam, results/AN/target/QC/target.QC.snplist
    shell:
        
        /proj/htzhu/UKB_GWAS/phase1and2/plink           --bfile data/target/genotype/ukb_imp_allchr_v3_40k_fmri           --maf 0.01           --hwe 1e-6           --geno 0.01           --mind 0.01           --write-snplist           --make-just-fam           --out results/AN/target/QC/target.QC
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Complete log: /work/users/s/u/sunzehui/PRS_pipeline/.snakemake/log/2024-06-17T140158.273734.snakemake.log
